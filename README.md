# about-sim

### Stanford dl4nlp lecture note
* [Lecture note 1](http://cs224d.stanford.edu/lecture_notes/notes1.pdf)
* [Lecture note 2](http://cs224d.stanford.edu/lecture_notes/notes2.pdf)
* [Lecture note 3](http://cs224d.stanford.edu/lecture_notes/notes3.pdf)
* [Lecture note 4](http://cs224d.stanford.edu/lecture_notes/notes4.pdf)
* [Lecture note 5](http://cs224d.stanford.edu/lecture_notes/LectureNotes5.pdf)

### Machine learning Basic
* [Note for Backpropagation](https://www.ics.uci.edu/~pjsadows/notes.pdf)
* [세상의 (거의) 모든 머신러닝 문제 공략법](http://keunwoochoi.blogspot.sg/2016/08/blog-post.html)
* [쉽게쓰여진 MCMC](http://blog.naver.com/rupy400/220775812498)
* [Optimization: Stochastic Gradient Descent](http://cs231n.github.io/optimization-1/#analytic)] [[note]()]
* [BackPropagation Through Time](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf)

### word2vec
* [딥러닝, NLP, 표현(Deep Learning, NLP, and Representations)](http://blog.naver.com/rupy400/220788129840) 
* [woer2vec explained](http://arxiv.org/pdf/1402.3722v1.pdf)
* [word2vec Parameter Learning Explained](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf) [[note](https://www.evernote.com/shard/s29/sh/e0af0845-ff9a-485c-a919-2d5d476f0a56/e22b7ece75f76ce8a3e0d469f78ef131)]
* [Query2Vec: Learning Deep Intentions from Heterogenous Search Logs](http://www.cs.cmu.edu/~dongyeok/papers/query2vec_v0.2.pdf)
* [Search Retargeting using Directed Query Embeddings](http://astro.temple.edu/~tua95067/grbovic2015wwwA.pdf)
* [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)
* [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf)
* [Improving Word Representations via Global Context and Multiple Word Prototypes](http://www.aclweb.org/anthology/P12-1092)
* Bag of Tricks for Efficient Text Classification [pdf](https://arxiv.org/pdf/1607.01759v3.pdf) [note](https://www.evernote.com/shard/s29/sh/8c779373-29d7-46f7-aa1d-929feecd3867/8fef8f3a842d2a64b4e16cfa66a3a775)

### Deep Neural Networks
* [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf) [[note](https://www.evernote.com/shard/s29/sh/4b34b0c8-0a51-4c1a-bea9-ffd7d5c80f8c/48e109cd29c4f767a68aa3be37584252)] [[요약](http://keunwoochoi.blogspot.sg/2016/09/deep-neural-networks-for-youtube.html)]


### Convolutional neural networks
* [콘볼루션 넷: 모듈 관점 (Conv Nets: A Modular Perspective)](http://blog.naver.com/rupy400/220788125109)  
* [IMPLEMENTING A CNN FOR TEXT CLASSIFICATION IN TENSORFLOW (한글 번역)](http://blog.naver.com/rupy400/220777178142) 
* [자연어 처리 문제를 해결하는 CONVOLUTIONAL NEURAL NETWORKS 이해하기](http://blog.naver.com/rupy400/220776488979)  
* [CS231n: Convolutional Neural Networks for Visual Recognition](http://aikorea.org/cs231n/)
* [CONNECTING IMAGES AND NATURAL LANGUAGE](http://cs.stanford.edu/people/karpathy/main.pdf)

### Recurrent neural networks
* [Recurrent Neural Network (RNN) Tutorial - Part 1](http://aikorea.org/blog/rnn-tutorial-1/)
* [RNN Tutorial Part 2 - Python, NumPy와 Theano로 RNN 구현하기](http://aikorea.org/blog/rnn-tutorial-2/)[[note](https://www.evernote.com/shard/s29/sh/a9afa6e5-7625-48d3-aebf-c5b1d365f963/109374ba7b27907431c8829469a9f56f)]
* [RNN Tutorial Part 3 - BPTT와 Vanishing Gradient 문제](http://aikorea.org/blog/rnn-tutorial-3/)
* * [RNN Tutorial Part 4 - GRU/LSTM RNN 구조를 Python과 Theano를 이용하여 구현하기](http://aikorea.org/blog/rnn-tutorial-4/)
* [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
* [EXTENSIONS OF RECURRENT NEURAL NETWORK LANGUAGE MODEL](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf) 
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
 * [한글번역](https://tgjeon.github.io/post/rnns-in-tensorflow/ )
* [RNNS IN TENSORFLOW, A PRACTICAL GUIDE AND UNDOCUMENTED FEATURES](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)
* [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  * 한글번역 http://www.whydsp.org/280

### Restricted Boltzmann Machines
* [제한 볼츠만 머신 초보자 튜토리얼](http://blog.naver.com/rupy400/220793514761)
* [RBM 이해하기](https://kyulabs.blogspot.kr/2016/08/rbm.html)
* [딥 오토인코더](https://kyulabs.blogspot.kr/2016/08/blog-post_14.html)
* [Autoencoder vs RBM (+ vs CNN)](http://khanrc.tistory.com/entry/Autoencoder-vs-RBM-vs-CNN)

## Vector search 
* [An Investigation of Practical Approximate
Nearest Neighbor Algorithms](http://www.cs.cmu.edu/~agray/approxnn.pdf)

### deeplearning4j
* [Deeplearning4j 소개](http://deeplearning4j.org/kr-index.html)
* [Deeplearning4j 퀵스타트 가이드](http://deeplearning4j.org/kr-quickstart)
* [아이겐벡터, 공분산, 주성분분석(PCA), 엔트로피의 기초](http://deeplearning4j.org/kr-eigenvector)
* [심층 신경망 (딥 뉴럴넷) 소개](http://deeplearning4j.org/kr-neuralnet-overview.html)
* [컨볼루션 신경망(뉴럴 네트워크)](http://deeplearning4j.org/kr-convolutionnets)
* [LSTM 자세한 튜토리알!](http://deeplearning4j.org/kr-lstm.html) 
* [Deeplearning4j의 RNN 모델 사용방법](http://deeplearning4j.org/kr-usingrnns.html)
* [자연어처리 : word2vec](http://deeplearning4j.org/kr-word2vec)
* [제한 볼츠만 머신 초보자 메뉴얼](http://blog.naver.com/rupy400/220793514761)
* [딥 오토인코더](https://kyulabs.wordpress.com/2016/08/21/%EB%94%A5-%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94/)

### tensorflow
* [Large-Scale Machine Learning on Heterogeneous Distributed Systems](http://download.tensorflow.org/paper/whitepaper2015.pdf)
* https://github.com/aymericdamien/TensorFlow-Examples
* [Tensorflow for machine intelligence](https://www.evernote.com/shard/s29/sh/3b83f498-3549-48c4-9a40-5119a58c2788/295769c108025ccdf29e7b74b017105c)

### distributed system
* [Large Scale Distributed Deep Networks](https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf)
